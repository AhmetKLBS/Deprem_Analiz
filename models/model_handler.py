# -*- coding: utf-8 -*-
import numpy as np
import pandas as pd
import joblib
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler
from datetime import datetime, timedelta
import requests

class EarthquakePredictor:
    def __init__(self, config):
        self.config = config
        self.model = config.model
        self.scaler = self._load_scaler()
        self.resolution = config.get("graphical_geometry", 1775)
        self.feature_columns = [
            'magnitude', 
            'depth_km', 
            'distance_to_coast',
            'historic_activity'
        ]

    def _load_scaler(self):
        """Eƒüitilmi≈ü scaler'ƒ± y√ºkler"""
        try:
            return joblib.load("models/scaler.joblib")
        except FileNotFoundError:
            print("‚ö†Ô∏è Scaler bulunamadƒ±, yeni bir scaler olu≈üturuluyor...")
            return StandardScaler()

    def fetch_realtime_data(self):
        """USGS'den ger√ßek zamanlƒ± veri √ßeker"""
        params = {
            "format": "geojson",
            "starttime": (datetime.utcnow() - timedelta(days=1)).strftime("%Y-%m-%d"),
            "endtime": datetime.utcnow().strftime("%Y-%m-%d"),
            "minmagnitude": self.config.get("min_magnitude", 2.0),
            "bbox": self.config.get("turkey_bbox", "35.5,25.5,42.5,44.5")
        }
        
        try:
            response = requests.get(
                self.config.get("usgs_api"),
                params=params,
                timeout=10
            )
            return self._parse_data(response.json()['features'])
        except Exception as e:
            print(f"üì° Veri √ßekme hatasƒ±: {str(e)}")
            return pd.DataFrame()

    def _parse_data(self, features):
        """Ham USGS verisini i≈üler"""
        processed = []
        for feature in features:
            props = feature['properties']
            coords = feature['geometry']['coordinates']
            processed.append({
                'timestamp': datetime.utcfromtimestamp(props['time']/1000),
                'latitude': coords[1],
                'longitude': coords[0],
                'depth_km': coords[2],
                'magnitude': props['mag'],
                'location': props['place']
            })
        return pd.DataFrame(processed)

    def predict_risk(self, quake_data):
        """Tek deprem i√ßin risk analizi yapar"""
        try:
            processed = self._process_features(quake_data)
            proba = self.model.predict_proba([processed])[0][1]
            return {
                'risk': round(proba, 3),
                'alert': proba > self.config.get("alert_threshold", 0.65),
                'resolution': self.resolution
            }
        except Exception as e:
            print(f"üîÆ Tahmin hatasƒ±: {str(e)}")
            return None

    def _process_features(self, data):
        """Veriyi model formatƒ±na d√∂n√º≈üt√ºr√ºr"""
        features = [
            data['magnitude'],
            data['depth_km'],
            self._coast_distance(data['latitude'], data['longitude']),
            self._historic_activity(data['latitude'], data['longitude'])
        ]
        return self.scaler.transform([features])[0]

    def _coast_distance(self, lat, lon):
        """Kƒ±yƒ±ya mesafe (basitle≈ütirilmi≈ü)"""
        return abs(lat - 36.0) + abs(lon - 30.0)  # Antalya referans noktasƒ±

    def _historic_activity(self, lat, lon):
        """Tarihsel deprem aktivitesi"""
        return 0.75 if lon > 33.0 else 0.25  # Doƒüu-batƒ± ayrƒ±mƒ±

    def optimize_visualization(self, raw_data):
        """Grafik √ß√∂z√ºn√ºrl√ºƒü√ºne g√∂re veriyi optimize eder"""
        if raw_data.empty:
            return pd.DataFrame()
            
        step_size = max(1, int(len(raw_data) / (self.resolution / 1000)))
        return raw_data.iloc[::step_size]

def train_model():
    """Model eƒüitim ve kaydetme fonksiyonu"""
    from sklearn.model_selection import train_test_split
    
    # √ñrnek veri √ºretimi
    X = np.random.rand(10000, 4) * 10  #magnitude, depth, distance, activity
    y = np.where(X[:,0] > 5.0, 1, 0)  # 5.0+ b√ºy√ºkl√ºk riskli kabul edilsin
    
    # Veri √∂l√ßeklendirme
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    # Model eƒüitimi
    model = RandomForestClassifier(
        n_estimators=200,
        max_depth=12,
        class_weight='balanced'
    )
    model.fit(X_scaled, y)
    
    # Kaydetme
    joblib.dump(model, "models/tr_earthquake_model.joblib")
    joblib.dump(scaler, "models/scaler.joblib")
    print("‚úÖ Model ba≈üarƒ±yla eƒüitildi ve kaydedildi")

if __name__ == "__main__":
    train_model()